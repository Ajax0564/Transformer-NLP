{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport torch\nfrom torch.utils.data import Dataset\nimport pickle\nimport os\nimport time \nfrom time import sleep\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom logging import Logger as logger\nimport warnings\nimport gc\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.simplefilter('ignore')\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead\nfrom tqdm.notebook import tqdm\nfrom transformers import TextDataset,DataCollatorForLanguageModeling,AutoConfig","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:42:05.919833Z","iopub.execute_input":"2024-03-10T14:42:05.920701Z","iopub.status.idle":"2024-03-10T14:42:16.738887Z","shell.execute_reply.started":"2024-03-10T14:42:05.920664Z","shell.execute_reply":"2024-03-10T14:42:16.737808Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"m = AutoModelWithLMHead.from_pretrained(\"../input/transformer-distilation-gpt-2/gpt2_6L\")\nconfig = AutoConfig.from_pretrained('../input/transformer-distilation-gpt-2/gpt2_6L')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:42:16.740736Z","iopub.execute_input":"2024-03-10T14:42:16.741366Z","iopub.status.idle":"2024-03-10T14:42:21.876350Z","shell.execute_reply.started":"2024-03-10T14:42:16.741336Z","shell.execute_reply":"2024-03-10T14:42:21.875379Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"state_dict = m.state_dict()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:42:21.877519Z","iopub.execute_input":"2024-03-10T14:42:21.877805Z","iopub.status.idle":"2024-03-10T14:42:21.883864Z","shell.execute_reply.started":"2024-03-10T14:42:21.877779Z","shell.execute_reply":"2024-03-10T14:42:21.882940Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"string = open('../input/mark-twain-books/MarkTwain_9_clean.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('�', '', string)\nopen('Test.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:42:21.886358Z","iopub.execute_input":"2024-03-10T14:42:21.886667Z","iopub.status.idle":"2024-03-10T14:42:21.910489Z","shell.execute_reply.started":"2024-03-10T14:42:21.886642Z","shell.execute_reply":"2024-03-10T14:42:21.909652Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"136454"},"metadata":{}}]},{"cell_type":"code","source":"string = open('/kaggle/input/mark-twain-books/Combine.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('�', '', string)\nopen('Train.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:42:21.911663Z","iopub.execute_input":"2024-03-10T14:42:21.912014Z","iopub.status.idle":"2024-03-10T14:42:21.995869Z","shell.execute_reply.started":"2024-03-10T14:42:21.911964Z","shell.execute_reply":"2024-03-10T14:42:21.994862Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6588596"},"metadata":{}}]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,config,device):\n        super(MultiHeadAttention,self).__init__()\n        self.n_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.hidden_size//self.n_heads\n        self.q = nn.Linear(self.hidden_size,self.hidden_size)\n        self.k = nn.Linear(self.hidden_size,self.hidden_size)\n        self.v = nn.Linear(self.hidden_size,self.hidden_size)\n        self.device = device\n\n        self.fc = nn.Linear(self.hidden_size,self.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(self.device)\n\n    def forward(self, query, key, value, mask = None):\n        batch_size = query.shape[0]\n        Q = self.q(query)\n        K = self.k(key)\n        V = self.v(value)\n        # [batch size, query len, hid dim]\n        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        x = F.scaled_dot_product_attention(Q,K,V,dropout_p=0.1,is_causal = True)\n#         # [batch size, n heads, query len, head dim]\n#         score = torch.matmul(Q, K.permute(0, 1, 3, 2)) /self.scale\n#         if mask is not None:\n#             score = score.masked_fill(mask == 0, -1e10)\n#         attention = torch.softmax(score, dim = -1)\n#         x = torch.matmul(self.dropout(attention), V)\n        x = x.permute(0, 2, 1, 3).contiguous()\n        \n        #x = [batch size, query len, n heads, head dim]\n        \n        x = x.view(batch_size, -1, self.hidden_size)\n        \n        #x = [batch size, query len, hid dim]\n        \n        x = self.fc(x)\n        return x\n    \nclass PositionwiseFeedforwardLayer(nn.Module):\n    def __init__(self,config):\n        super(PositionwiseFeedforwardLayer,self).__init__()\n        self.pf_dim = 3072\n        self.hid_dim = config.hidden_size\n        self.fc_1 = nn.Linear(self.hid_dim, self.pf_dim)\n        self.fc_2 = nn.Linear(self.pf_dim, self.hid_dim)\n        self.dropout = nn.Dropout(0.1)\n        self.activation = nn.GELU()\n        \n    def forward(self, x):\n        \n        #x = [batch size, seq len, hid dim]\n        \n        x = self.dropout(self.activation(self.fc_1(x)))\n        \n        #x = [batch size, seq len, pf dim]\n        \n        x = self.fc_2(x)\n        \n        #x = [batch size, seq len, hid dim]\n        \n        return x\n    \nclass DecoderLayer(nn.Module):\n    def __init__(self,config,device):\n        super(DecoderLayer,self).__init__()\n        self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.enc_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.ff_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.self_attention = MultiHeadAttention(config, device)\n        self.positionwise_feedforward = PositionwiseFeedforwardLayer(config)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, trg,trg_mask):\n        \n        #trg = [batch size, trg len, hid dim]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n        \n        #self attention\n        _trg = self.self_attention(trg, trg, trg, trg_mask)\n        \n        #dropout, residual connection and layer norm\n        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n            \n        #trg = [batch size, trg len, hid dim]\n            \n        #encoder attention cross attention\n    \n        \n        #dropout, residual connection and layer norm\n#         trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n                    \n        #trg = [batch size, trg len, hid dim]\n        \n        #positionwise feedforward\n        _trg = self.positionwise_feedforward(trg)\n        \n        #dropout, residual and layer norm\n        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        return trg\n    \nclass Decoder(nn.Module):\n    def __init__(self,config,device):\n        super(Decoder,self).__init__()\n        self.device = device\n        self.vocab = config.vocab_size\n        self.hid_dim = config.hidden_size\n        self.max_length = config.max_position_embeddings\n        self.tok_embedding =nn.Embedding.from_pretrained(state_dict['transformer.wte.weight'], freeze=False)                    #nn.Embedding(self.vocab, self.hid_dim)\n        self.pos_embedding = nn.Embedding.from_pretrained(state_dict['transformer.wpe.weight'], freeze=False)                     #nn.Embedding(self.max_length, self.hid_dim)\n        self.n_layers = 6 #config.num_hidden_layers\n        self.layers = nn.ModuleList([DecoderLayer(config,device) \n                                     for _ in range(self.n_layers)])\n        self.dropout = nn.Dropout(0.1)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([config.hidden_size])).to(device)\n#         self.dropout = nn.Dropout(0.1)\n        self.fc_out = nn.Linear(self.hid_dim, self.vocab)\n        \n    def make_trg_mask(self,trg):\n        \n        \n        #trg_pad_mask = [batch size, 1, 1, trg len]\n\n#         trg_pad_mask = (trg).unsqueeze(1).unsqueeze(2)  #No padding#!= tokenizer.pad_token_id\n        \n        trg_len = trg.shape[1]\n        \n        trg_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n#         print(trg_pad_mask.size(),trg_sub_mask.size())\n        #trg_sub_mask = [trg len, trg len]\n            \n#         trg_mask = trg_pad_mask & trg_sub_mask\n        \n        #trg_mask = [batch size, 1, trg len, trg len]\n        \n        return trg_mask\n    \n    def forward(self, trg):\n        \n        #trg = [batch size, trg len]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n                \n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        \n        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n                            \n        #pos = [batch size, trg len]\n            \n        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n        trg_mask = self.make_trg_mask(trg)      \n        #trg = [batch size, trg len, hid dim]\n        \n        for layer in self.layers:\n            trg = layer(trg,trg_mask)\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        output = self.fc_out(trg)\n        \n        #output = [batch size, trg len, output dim]\n            \n        return output\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:11.168272Z","iopub.execute_input":"2024-03-10T14:43:11.168732Z","iopub.status.idle":"2024-03-10T14:43:11.209324Z","shell.execute_reply.started":"2024-03-10T14:43:11.168692Z","shell.execute_reply":"2024-03-10T14:43:11.208270Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model  = Decoder(config,device)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:12.071205Z","iopub.execute_input":"2024-03-10T14:43:12.071581Z","iopub.status.idle":"2024-03-10T14:43:12.941506Z","shell.execute_reply.started":"2024-03-10T14:43:12.071549Z","shell.execute_reply":"2024-03-10T14:43:12.940406Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#  = [3,324]\n# model(x).size\n# torch.Size([3, 324, 50257])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:13.024639Z","iopub.execute_input":"2024-03-10T14:43:13.025117Z","iopub.status.idle":"2024-03-10T14:43:13.031217Z","shell.execute_reply.started":"2024-03-10T14:43:13.025082Z","shell.execute_reply":"2024-03-10T14:43:13.030034Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntrain_path = 'Train.txt'\ntest_path = 'Test.txt'","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:13.993281Z","iopub.execute_input":"2024-03-10T14:43:13.994104Z","iopub.status.idle":"2024-03-10T14:43:14.352715Z","shell.execute_reply.started":"2024-03-10T14:43:13.994059Z","shell.execute_reply":"2024-03-10T14:43:14.351684Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(\n        self,\n        tokenizer,\n        file_path: str,\n        block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n\n        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n        saved = False\n        cache_dir = None\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            cache_dir if cache_dir is not None else directory,\n            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n        )\n\n     \n        if os.path.exists(cached_features_file) and saved :\n                start = time.time()\n                with open(cached_features_file, \"rb\") as handle:\n                    self.examples = pickle.load(handle)\n#                 logger.info(\n#                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n#                 )\n\n        else:\n#                 logger.info(f\"Creating features from dataset file at {directory}\")\n\n                self.examples = []\n                with open(file_path, encoding=\"utf-8\") as f:\n                    text = f.read()\n\n                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\n                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n                    self.examples.append(\n                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n                    )\n                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n                # If your dataset is small, first you should look for a bigger one :-) and second you\n                # can change this behavior by adding (model specific) padding.\n\n                start = time.time()\n                with open(cached_features_file, \"wb\") as handle:\n                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                    saved = True\n#                 logger.info(\n#                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n#                 )\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> torch.Tensor:\n        return {\"input_ids\":torch.tensor(self.examples[i], dtype=torch.long)}","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:14.890824Z","iopub.execute_input":"2024-03-10T14:43:14.891976Z","iopub.status.idle":"2024-03-10T14:43:14.904741Z","shell.execute_reply.started":"2024-03-10T14:43:14.891922Z","shell.execute_reply":"2024-03-10T14:43:14.903750Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def collate(batch):\n    labels = batch[\"input_ids\"].clone()\n    if tokenizer.pad_token_id is not None:\n        labels[labels == tokenizer.pad_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:15.525240Z","iopub.execute_input":"2024-03-10T14:43:15.525602Z","iopub.status.idle":"2024-03-10T14:43:15.531254Z","shell.execute_reply.started":"2024-03-10T14:43:15.525572Z","shell.execute_reply":"2024-03-10T14:43:15.530134Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(TextDataset(tokenizer,train_path,128),batch_size=24, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(TextDataset(tokenizer,test_path,128),batch_size=24, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:16.128396Z","iopub.execute_input":"2024-03-10T14:43:16.128771Z","iopub.status.idle":"2024-03-10T14:43:25.337575Z","shell.execute_reply.started":"2024-03-10T14:43:16.128741Z","shell.execute_reply":"2024-03-10T14:43:25.336731Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"no_decay = ['bias', 'LayerNorm.weight','LayerNorm.bias']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=3e-4)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:25.339394Z","iopub.execute_input":"2024-03-10T14:43:25.339689Z","iopub.status.idle":"2024-03-10T14:43:25.349699Z","shell.execute_reply.started":"2024-03-10T14:43:25.339663Z","shell.execute_reply":"2024-03-10T14:43:25.348731Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\naccumulation_steps = 1\nnum_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n                                    num_training_steps=num_train_optimization_steps)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:25.351030Z","iopub.execute_input":"2024-03-10T14:43:25.351392Z","iopub.status.idle":"2024-03-10T14:43:25.362511Z","shell.execute_reply.started":"2024-03-10T14:43:25.351355Z","shell.execute_reply":"2024-03-10T14:43:25.361606Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def loss_fn(labels,prediction_scores):\n    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n    labels = labels[:, 1:].contiguous()\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    lm_loss = loss_fct(shifted_prediction_scores.view(-1, config.vocab_size), labels.view(-1))\n    return lm_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:25.364228Z","iopub.execute_input":"2024-03-10T14:43:25.364569Z","iopub.status.idle":"2024-03-10T14:43:25.372179Z","shell.execute_reply.started":"2024-03-10T14:43:25.364524Z","shell.execute_reply":"2024-03-10T14:43:25.371407Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def valid_func(model,valid_loader):\n    model.eval()\n    bar = tqdm(valid_loader,file=sys.stdout)\n#     loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    losses = []\n    with torch.no_grad():\n        for batch_idx, (data) in enumerate(bar):\n            data =  collate(data)\n            x = data[\"input_ids\"].to(device)\n            y = data['labels'].to(device)\n            pred = model(x)  \n            \n            loss = loss_fn(y,pred)\n            losses.append(loss.item())\n           \n            bar.set_description(f'loss: {loss.item():.5f}')\n   \n    loss_valid =  np.round(np.mean(losses),4)\n    return loss_valid","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:25.373215Z","iopub.execute_input":"2024-03-10T14:43:25.373496Z","iopub.status.idle":"2024-03-10T14:43:25.382735Z","shell.execute_reply.started":"2024-03-10T14:43:25.373472Z","shell.execute_reply":"2024-03-10T14:43:25.381951Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import sys\nbest_epoch_loss = np.inf\nmodel.to(device)\nfor epoch in range(EPOCHS):\n    start_time = time.time()\n    avg_loss = 0.0\n    model.train()\n    tbar = tqdm(train_loader, file=sys.stdout)\n    loss_list = []\n    tbar.set_description(f\"Epoch {epoch+1}\")\n    for step, data in enumerate(tbar):\n        data =  collate(data)\n        x = data[\"input_ids\"].to(device)\n        y = data['labels'].to(device)\n        optimizer.zero_grad()\n        pred = model(x)\n        loss = loss_fn(y,pred)\n#         loss = loss_fn(pred, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        tbar.set_postfix(loss=loss.item())\n#         sleep(0.1)\n        loss_list.append(loss.detach().cpu().item())\n    avg_loss = np.round(np.mean(loss_list), 4)\n#     tbar.set_description(f\"Epoch {epoch + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n#     vloss = valid_func(model,val_loader)\n#     log_df.loc[len(log_df.index)] = [epoch+1,avg_loss,vloss]\n    print(f'Epoch--{epoch+1} ### Train loss---{avg_loss}')\n#     if (step%200)==0:\n#         print(f'Train_loss={avg_loss}')\n#     if vloss<best_epoch_loss:\n#         best_epoch_loss = vloss\n    PATH = f\"gpt2_epoch__{epoch}.pth\"\n#         model.save_pretrained(PATH)\n    torch.save(model.state_dict(), PATH)\n#         print(f'Model Saved--epoch--{epoch+1}')\n        \n    \ndel train_loader\ndel model\ndel val_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:43:27.991150Z","iopub.execute_input":"2024-03-10T14:43:27.992051Z","iopub.status.idle":"2024-03-10T15:07:54.756807Z","shell.execute_reply.started":"2024-03-10T14:43:27.992007Z","shell.execute_reply":"2024-03-10T15:07:54.755566Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b3a3c4ca8d4eeda33338bcd09abdb8"}},"metadata":{}},{"name":"stdout","text":"Epoch--1 ### Train loss---6.2484\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a042ce7b3dd84207a8a37cf78939a288"}},"metadata":{}},{"name":"stdout","text":"Epoch--2 ### Train loss---4.8753\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e233e366214f749940af04c9170e83"}},"metadata":{}},{"name":"stdout","text":"Epoch--3 ### Train loss---4.4331\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c09069815904609a0e7894e7fa09346"}},"metadata":{}},{"name":"stdout","text":"Epoch--4 ### Train loss---4.0991\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9cf77b3d5ba485a9dd100c9e2d1134e"}},"metadata":{}},{"name":"stdout","text":"Epoch--5 ### Train loss---3.8031\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3ddb0de100d470dbe475944eb8cda19"}},"metadata":{}},{"name":"stdout","text":"Epoch--6 ### Train loss---3.5282\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78824c64968041d4b62fad4008086a0c"}},"metadata":{}},{"name":"stdout","text":"Epoch--7 ### Train loss---3.2725\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdfcc2661f154e6e9c13df978f4d35cb"}},"metadata":{}},{"name":"stdout","text":"Epoch--8 ### Train loss---3.044\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d58565772b7480493d45a7f490c2eb6"}},"metadata":{}},{"name":"stdout","text":"Epoch--9 ### Train loss---2.8537\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cfa112ae90c4244afeab1b78e515c8a"}},"metadata":{}},{"name":"stdout","text":"Epoch--10 ### Train loss---2.7171\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"12855"},"metadata":{}}]},{"cell_type":"code","source":"# !ls ../working/gpt2_epoch__2.pth","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:54.758546Z","iopub.execute_input":"2024-03-10T15:07:54.758865Z","iopub.status.idle":"2024-03-10T15:07:54.763418Z","shell.execute_reply.started":"2024-03-10T15:07:54.758836Z","shell.execute_reply":"2024-03-10T15:07:54.762432Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = Decoder(config,device)\nmodel.load_state_dict(torch.load('../working/gpt2_epoch__9.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:54.764525Z","iopub.execute_input":"2024-03-10T15:07:54.764802Z","iopub.status.idle":"2024-03-10T15:07:56.026964Z","shell.execute_reply.started":"2024-03-10T15:07:54.764778Z","shell.execute_reply":"2024-03-10T15:07:56.026023Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:56.029543Z","iopub.execute_input":"2024-03-10T15:07:56.030191Z","iopub.status.idle":"2024-03-10T15:07:56.156586Z","shell.execute_reply.started":"2024-03-10T15:07:56.030162Z","shell.execute_reply":"2024-03-10T15:07:56.155656Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Decoder(\n  (tok_embedding): Embedding(50257, 768)\n  (pos_embedding): Embedding(1024, 768)\n  (layers): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (enc_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (ff_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (self_attention): MultiHeadAttention(\n        (q): Linear(in_features=768, out_features=768, bias=True)\n        (k): Linear(in_features=768, out_features=768, bias=True)\n        (v): Linear(in_features=768, out_features=768, bias=True)\n        (fc): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n        (fc_1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc_2): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc_out): Linear(in_features=768, out_features=50257, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def generate(text, max_new_tokens=512, temperature=1.0, do_sample=True, top_k=10):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        idx = tokenizer.encode(text,add_special_tokens=False, return_tensors=\"pt\")\n        idx = idx.to(device)\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= 128 else idx[:, -128:]\n            # forward the model to get the logits for the index in the sequence\n            logits = model(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, top_k)\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # either sample from the distribution or take the most likely element\n            if do_sample:\n                idx_next = torch.multinomial(probs, num_samples=1)\n            else:\n                _, idx_next = torch.topk(probs, k=1, dim=-1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:56.157764Z","iopub.execute_input":"2024-03-10T15:07:56.158069Z","iopub.status.idle":"2024-03-10T15:07:56.167377Z","shell.execute_reply.started":"2024-03-10T15:07:56.158043Z","shell.execute_reply":"2024-03-10T15:07:56.166565Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"out = generate('this of the test')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:56.168519Z","iopub.execute_input":"2024-03-10T15:07:56.168793Z","iopub.status.idle":"2024-03-10T15:08:00.030063Z","shell.execute_reply.started":"2024-03-10T15:07:56.168769Z","shell.execute_reply":"2024-03-10T15:08:00.029275Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# out","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:08:00.031467Z","iopub.execute_input":"2024-03-10T15:08:00.031737Z","iopub.status.idle":"2024-03-10T15:08:00.035891Z","shell.execute_reply.started":"2024-03-10T15:08:00.031714Z","shell.execute_reply":"2024-03-10T15:08:00.034970Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(out[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:08:00.036942Z","iopub.execute_input":"2024-03-10T15:08:00.037251Z","iopub.status.idle":"2024-03-10T15:08:00.049635Z","shell.execute_reply.started":"2024-03-10T15:08:00.037227Z","shell.execute_reply":"2024-03-10T15:08:00.048758Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"\"this of the test that can be a  noble soul, but you can do it.It shall be hard.I think it  of a man to make the most men that do the most noble and wise  admirers.The man who could not live to be hanged, said to them:  I have seen a good idea that they were a very person, and  you should say that that the king would be an ass who would be his  best, and so we might do it.I will say that a few moments  I have seen him, and then I will not tell you what he did.  The gentleman and his sons were full of self-complacent and gentle folk,  and were his work.He was very well, of course, and I mean, but  that they couldn't stand that man; and he couldn't seem as to have  been a person's.He looked up in his mind and said, but said,  I didn't seem to get him around him at all--I thought  I was ashamed of him to have him a given him, and asked him if he  would not have him.But that he couldn't have the trouble with us,  but he didn't quite what we were about.  He didn't.  I knew the matter what he meant a person we had, and that it was  that we had a pleasant good time, but was it wouldn't  anybody else; it was natural.We had a good many years,  but we had had the same luck.They were in it was too late;  but that they didn't go on, they were not very good friends  because we were so poor that we were afraid we hadnt any chance to make  them get them.And we had them to give us the rest; we  had them that made them run their business and had; and they could  go to a-day and see if they had been good, and they didnt  know the way, but they had all their own way, and I judged it  had a sudden mind, because it warnt a thing like that. And we  wanted to know it, and so we _well_ was so; we couldnt_ see  nothing, and they was so; and I wanted to see what we might be, but  _did_, too, and we couldnt.  We turned down, and down, and Jim Lane, he says:  _Now_  Whats dey? Its a\""},"metadata":{}}]},{"cell_type":"code","source":"out = generate('long before that I was')\ntokenizer.decode(out[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:08:00.050628Z","iopub.execute_input":"2024-03-10T15:08:00.050869Z","iopub.status.idle":"2024-03-10T15:08:03.873112Z","shell.execute_reply.started":"2024-03-10T15:08:00.050839Z","shell.execute_reply":"2024-03-10T15:08:03.872206Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'long before that I was  surprised to see a man, I had just as fresh a cat in a tree on the  floor of the ground that had been asleep, there was a man whose head  thawed his legs and his back. Then I said:  Now you know the marks of the first time you know it, and then Ive  got a been in my mind, Tom Sawyer.  Yes, he couldnt seem to tell about it now.  Well, that aint what to say, anyway. Well, then, when you was over, and  _you_ know a body, aint you ever so? Thats the way you  dont know. And I know about it a person it, I aint going to do  any of a robber. But its awful.  The professor was gone out of the place and held no longer, but that  didnt make no use. He was in the best, too, and I reckoned it would be a  bad little while, and you see it was just a little brick.  It was a mighty small boy, you know, and you know that if youve got to know the  thing you aint _do_?  You just waitIll tell me and see you do it, Tom said theres another thing youll be,  and that _you_ do, and you cant ever see how long, Huck?  I didnt say any good, but that I wouldnt see you, and you would  never have an idea of this trouble with your last night till youd  got out of it, and Ill go home, but if youll be more and  So I wouldnt get him up. He was a disgraceful of that moneyand if he wouldnt  drop it if I wouldnt ever tell _what_? And yet, if I hadnt ever been  a pirate, youd _d_ dont ever have any morenomer-and if hed been  better.  Well, if I didnt, Tom, I wouldnt _dont_ tell you? You _d never would  have a thousand times. Well, I reckon. But I dont want to tell anybody.  Why, you know that. You know, if it aint the use to run of  my head or I dont want to. You must go long to bed, youll get to  it.  So it was a long time, you wouldnt ever have to wait for anybody anywhere but  nothing, and you'"},"metadata":{}}]},{"cell_type":"code","source":"out = generate('Well, sir, you could')\ntokenizer.decode(out[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:08:03.876093Z","iopub.execute_input":"2024-03-10T15:08:03.876509Z","iopub.status.idle":"2024-03-10T15:08:07.668342Z","shell.execute_reply.started":"2024-03-10T15:08:03.876477Z","shell.execute_reply":"2024-03-10T15:08:07.667304Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"\"Well, sir, you could have thought  that this miracle is so; that's the only one thing that does not know just  what.It has been done a great many months since then, and  I'd like to know that, if you were, or what it can help you,  or you must it I think, or maybe; that's to give you the right one  and take the trouble.It's a good time to be used at a time like that for  other.  Now, what's a general thing?The king and a magician's.He must  get his money and give up the chance, and so, I could make  it out, too.I said:  There was nothing wrong about it to be so much to you.  The reason why should it be done in a business, that there's.  I was a good deal confused.I mean, now, you know, what you do.  Well, now, I had to make out this thing with what is the  method of a magician who is a common way; I know it may be  not; yet I mean; if there was an error in my hand I could have been  to keep it up _that_ I would.  Well, I didn't.I mean to say it; it was just a fact; that that  _can_ would be certain if you would kill _you_ was a slave  and a magician, and you couldn't know it, for you could  have it for that; and you didn't know, it was plain that  I wasnt a fool.The thing that made me mean; it wouldn't have  you do that--that is the same; it is the case, it is  no use; you couldn't understand, it wouldn't seem  necessary.I couldn't know a man; he didn't seem  to waste a magician in the same way.It was the  natural one, to see the people do not know it for  that; they all was that that one of the sort is not to  show-bred.  It was of a good sort.But, it was the one.I had another  sort of luck, but it was no doubt.I had to be punished.  I would not say it, I said; it didn't answer me.  --well, it didn't answer.I did.I couldn't seem to have it  for it myself; it couldn't waste any thing out of it\""},"metadata":{}}]},{"cell_type":"code","source":"out = generate('uncle into a thousand times')\ntokenizer.decode(out[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:08:07.669414Z","iopub.execute_input":"2024-03-10T15:08:07.669682Z","iopub.status.idle":"2024-03-10T15:08:11.705179Z","shell.execute_reply.started":"2024-03-10T15:08:07.669659Z","shell.execute_reply":"2024-03-10T15:08:11.704157Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"\"uncle into a thousand times more  --and I am satisfied again now--I mean to see you--and I do, don't like  any harm.You've got to go into a nice old age-turtle.  That's the very good. I don't quite think much of anything.  The old man, you're full of a hair--n't a hair off my ears, and a  minute's head's tail, and a little--he'll fetch it to the man's  and the old man, and he'll get it out on you till it goes  on his back and get the way.Well, it does seem so?  But I said the king.  That's a lie.  Well, I don't know, I didn't quite know.  Well, then, the man who can't go to the court that man that's  a-laughing and just at a time in time, and he's all at once  three days at a little time when that is finished.Now  I'll tell him what you've got to give you something to  give to give you a chance to do with that thing for?  You don't remember anything I've got a dirk, and he'll get out  and follow it; you'll get him the way you want to make him pay  it by one-dishing, or he can't.  You don't have a good time to get a chance, and I never see  a chance to-day if it would be the way you would be a poor fellow  and a man who would go about that way.  So you've got one kind of a nice many for him; that's the letter  of a friend--he's going to be a nice one. He wants to go and play  me and tell you a good man.  I'll tell you everything about the things you can do. You get up a little  man's work in him; I'll get to do this--.  It was in a letter home that year of mine--and he was always getting the  very first in his pocket, when he was in the matter with me he would have  been no longer, he wrote, or had been spared the money he will  have a good scheme to make him write as his books and write  over in the matter. He said that he could not write any book, but it  would never get the way to write.  You know by your written a book by\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
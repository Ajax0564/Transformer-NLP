{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom transformers import TextDataset,DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:29.274663Z","iopub.execute_input":"2023-05-16T17:36:29.275024Z","iopub.status.idle":"2023-05-16T17:36:40.857988Z","shell.execute_reply.started":"2023-05-16T17:36:29.274995Z","shell.execute_reply":"2023-05-16T17:36:40.856988Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nstring = open('../input/mark-twain-books/MarkTwain_9_clean.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('ï¿½', '', string)\nopen('Test.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:40.860085Z","iopub.execute_input":"2023-05-16T17:36:40.860825Z","iopub.status.idle":"2023-05-16T17:36:40.879175Z","shell.execute_reply.started":"2023-05-16T17:36:40.860787Z","shell.execute_reply":"2023-05-16T17:36:40.878225Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"136454"},"metadata":{}}]},{"cell_type":"code","source":"string = open('/kaggle/input/mark-twain-books/Combine.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('ï¿½', '', string)\nopen('Train.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:40.880720Z","iopub.execute_input":"2023-05-16T17:36:40.881065Z","iopub.status.idle":"2023-05-16T17:36:40.975575Z","shell.execute_reply.started":"2023-05-16T17:36:40.881034Z","shell.execute_reply":"2023-05-16T17:36:40.974692Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"6588596"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntrain_path = 'Train.txt'\ntest_path = 'Test.txt'    ","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:40.978098Z","iopub.execute_input":"2023-05-16T17:36:40.978471Z","iopub.status.idle":"2023-05-16T17:36:43.430829Z","shell.execute_reply.started":"2023-05-16T17:36:40.978439Z","shell.execute_reply":"2023-05-16T17:36:43.429904Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae52853564c48d19026fbad07f5a5e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730cf268ef784975a931f682e0bff7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc1bd0351414f3d8f909852a7b45c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d57bb166c541678ef0f9ccbe19c270"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(train_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=128)\n     \n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=128)   \n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,test_dataset,data_collator\n\ntrain_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:43.432348Z","iopub.execute_input":"2023-05-16T17:36:43.432693Z","iopub.status.idle":"2023-05-16T17:36:52.547048Z","shell.execute_reply.started":"2023-05-16T17:36:43.432661Z","shell.execute_reply":"2023-05-16T17:36:52.546046Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\nToken indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n\nmodel = AutoModelWithLMHead.from_pretrained(\"../input/transformer-distilation-gpt-2/gpt2_6L\")\n\n\ntraining_args = TrainingArguments(report_to='none',\n    output_dir=\"./gpt2-mark\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=3, # number of training epochs\n    per_device_train_batch_size=16, # batch size for training\n    per_device_eval_batch_size=16,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved \n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    prediction_loss_only=True,\n    )\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:36:52.548415Z","iopub.execute_input":"2023-05-16T17:36:52.549749Z","iopub.status.idle":"2023-05-16T17:37:02.961997Z","shell.execute_reply.started":"2023-05-16T17:36:52.549710Z","shell.execute_reply":"2023-05-16T17:37:02.961042Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:37:08.848891Z","iopub.execute_input":"2023-05-16T17:37:08.849748Z","iopub.status.idle":"2023-05-16T17:44:25.655956Z","shell.execute_reply.started":"2023-05-16T17:37:08.849712Z","shell.execute_reply":"2023-05-16T17:44:25.655089Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2316' max='2316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2316/2316 07:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>4.924500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.386400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.229800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.130300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2316, training_loss=4.372895657501484, metrics={'train_runtime': 436.7858, 'train_samples_per_second': 84.824, 'train_steps_per_second': 5.302, 'total_flos': 1210130576179200.0, 'train_loss': 4.372895657501484, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"def generate_messages(\n    model,\n    tokenizer,\n    prompt_text,\n    stop_token,\n    length,\n    num_return_sequences,\n    temperature = 0.7,\n    k=20,\n    p=0.9,\n    repetition_penalty = 1.0,\n    device = 'cuda'\n):\n\n    MAX_LENGTH = int(10000)\n    def adjust_length_to_model(length, max_sequence_length):\n        if length < 0 and max_sequence_length > 0:\n            length = max_sequence_length\n        elif 0 < max_sequence_length < length:\n            length = max_sequence_length  # No generation bigger than model size\n        elif length < 0:\n            length = MAX_LENGTH  # avoid infinite loop\n        return length\n        \n    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n\n    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n\n    encoded_prompt = encoded_prompt.to(device)\n\n    output_sequences = model.generate(\n            input_ids=encoded_prompt,\n            max_length=length + len(encoded_prompt[0]),\n            temperature=temperature,\n            top_k=k,\n            top_p=p,\n            repetition_penalty=repetition_penalty,\n            do_sample=True,\n            num_return_sequences=num_return_sequences,\n        )\n\n    if len(output_sequences.shape) > 2:\n        output_sequences.squeeze_()\n\n    generated_sequences = []\n\n    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n        generated_sequence = generated_sequence.tolist()\n\n        # Decode text\n        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n\n        # Remove all text after the stop token\n        text = text[: text.find(stop_token) if stop_token else None]\n\n        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n        total_sequence = (\n            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n        )\n\n        generated_sequences.append(total_sequence)\n    return generated_sequences\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:25.657956Z","iopub.execute_input":"2023-05-16T17:44:25.658438Z","iopub.status.idle":"2023-05-16T17:44:25.669711Z","shell.execute_reply.started":"2023-05-16T17:44:25.658403Z","shell.execute_reply":"2023-05-16T17:44:25.668743Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = AutoModelWithLMHead.from_pretrained('../working/gpt2-mark/checkpoint-1600')\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:25.670976Z","iopub.execute_input":"2023-05-16T17:44:25.671415Z","iopub.status.idle":"2023-05-16T17:44:27.157055Z","shell.execute_reply.started":"2023-05-16T17:44:25.671382Z","shell.execute_reply":"2023-05-16T17:44:27.156174Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"temperature = 1.0\nk=400\np=0.9\nrepetition_penalty = 1.0\nnum_return_sequences = 5\nlength = 100\nstop_token = '|EndOfText|'\nprompt_text = \"this is\"","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:27.159675Z","iopub.execute_input":"2023-05-16T17:44:27.160016Z","iopub.status.idle":"2023-05-16T17:44:27.165044Z","shell.execute_reply.started":"2023-05-16T17:44:27.159984Z","shell.execute_reply":"2023-05-16T17:44:27.164024Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"generate_messages(\n    model,\n    tokenizer,\n    prompt_text,\n    stop_token,\n    length,\n    num_return_sequences,\n    temperature = temperature,\n    k=k,\n    p=p,\n    repetition_penalty = repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:27.166629Z","iopub.execute_input":"2023-05-16T17:44:27.166991Z","iopub.status.idle":"2023-05-16T17:44:29.377096Z","shell.execute_reply.started":"2023-05-16T17:44:27.166958Z","shell.execute_reply":"2023-05-16T17:44:29.376161Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['this is not the same story, for he would  stay longer than half a week when he could be invited to return.  Livy and Becky were good friends of Tom Sawyer, and he slept  comfortably and slept during the two-night trip, not much larger than half an  year. He used to have visited the famous Villa Villa here several times before, and  and then the occasion came when he wished there was a time to know that the  guest of the Villa was sleeping, and ther',\n 'this is  a pity that is proper. So  I was gone for the coming. We couldnt see its two legs long. Still,  we had been around for a few years. It was an easy task, but  we had such a time. What would have happened, Clemens  knew it happened that Mark Twain would not stay anywhere near Mark Twain, but  will stay there while the last twilight; there was always one day, when he  saw Mark Twain enter on a blind alle',\n 'this is only just under the  influence of the reader.  CHAPTER XIV.  If the study did not spread over all the others, a heavy undertaking to report  what it would be known to be the worst heresy of its heresy; it must  be kept on account with a long line of arguments and arguments and philosophies which  were not always convincing enough to support with prejudice. The great Roman  S. R. N. Clemens, wrote in English and then arrived at a deep and',\n 'this is a delightful  readjust. It does not seem to have happened to me that one shall be in love with me.  He is half aware that he must have made for some of us, and that  the editor of it must have set himself up  to make our business record. It is not true to say that he has  made any definite agreement with us, but that is in truth. His satisfaction with us  that we had a perfect record of being a nation that made  ou',\n 'this is no more than  more complicated:  Huck, Huck, youre wasting a fortune upon it! I got one hand, and  with two guides, each with his hands folded; and a half of  the time we slipped on down the top of the high hill to stretch its  hush and drop it upon us, and then it was in the middle of a  pilchadentome. Then Tom stretched his trousers up. It  hurt him too, but it hurt hi']"},"metadata":{}}]},{"cell_type":"code","source":"generate_messages(\n    model,\n    tokenizer,\n    'long before that I was',\n    stop_token,\n    length,\n    num_return_sequences,\n    temperature = temperature,\n    k=k,\n    p=p,\n    repetition_penalty = repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:29.378656Z","iopub.execute_input":"2023-05-16T17:44:29.379030Z","iopub.status.idle":"2023-05-16T17:44:30.447390Z","shell.execute_reply.started":"2023-05-16T17:44:29.378993Z","shell.execute_reply":"2023-05-16T17:44:30.446302Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[\"long before that I was there.  On my face, in case of my secretary-of-the-office, there was a group of  professors who worked in her study on the natural  natural condition. I had gone away on the back end of a trip of three  months when I had never met any businessmen of any business in it; and I kept  about the most popular papers from place, which were some of the ones of the  present day's  news from the day to the end\",\n 'long before that I was very small. This  letter is all written to the press:  This letter is about the morning of September 18th. It was due when Mr.  W. Clemens brought it to the home of Mark Twain  and seemed to be most grateful for the occasion.  There was another one, a letter written on September 16, 1908, which is identical to that letter.  He wrote to Toms mouth, during the following day:  I have no more honor and appreciation for you',\n 'long before that I was able to see its way, I was  familiar with that old book, and the very picture of it is  fair enough, for when the war is over, I saw it very well in  a certain summer, and there was not the kind of a fair and cheerful  experience. And when the war is over, you see, and you do it yourself  to know exactly where that is. For a while of  years the moment we entered in this town a poor  white man stoo',\n 'long before that I was going to fly down the river for any  reason. I did have an anxiety of getting to sleep.  The day or week-long absence was on the table when I thought of it. I thought  to him, but my mind wandered into those unfamiliar woods, and I took a moment  into a drowsy and apprehensive look, and drifted along along  a limb with a tearful silence. I was  going to go on sleep, and I took a minute  again t',\n 'long before that I was lying awake.  Now, you can see the church-yard from the point you  see it. It was a pretty good way to leave this place and all went straight out  with a quiet oesophagus to sit  there, when I got to work, and was in the right place a sort of  sort of low fog that was drifting in the undergrowth a dimtured,  gray, gray, thick, cloudy sandal, in a fog that appeared to flicke']"},"metadata":{}}]},{"cell_type":"code","source":"generate_messages(\n    model,\n    tokenizer,\n    'Well, sir, you could',\n    stop_token,\n    length,\n    num_return_sequences,\n    temperature = temperature,\n    k=k,\n    p=p,\n    repetition_penalty = repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:30.448955Z","iopub.execute_input":"2023-05-16T17:44:30.449340Z","iopub.status.idle":"2023-05-16T17:44:31.514457Z","shell.execute_reply.started":"2023-05-16T17:44:30.449306Z","shell.execute_reply":"2023-05-16T17:44:31.513441Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[\"Well, sir, you couldnt be expected to hear that it was a matter of fact; but  that way, Huck said. You'll say it in a  little way, Huck, then, Tom?  Well, what do you want to play about it.  Well, fine, Huck. Well, it aint no use for getting that.  There was a room to play, Huck. Well, you'd like to  play like that, and Tom asked the guests to come,  and put o\",\n \"Well, sir, you couldnt come here, for I can't make yourself  out of it.  When I was there I saw a tall, tall, clean-up house where the storm  poured out, and I was pretty sure to have him; he said he was sitting out  before you, and did the rain fall off, too.  How can you keep yourself in that?  So the dust fell on its walls and dropped out all the time.  It was cold, and every time I sa\",\n 'Well, sir, you couldnt get you the doctor  yet, said Uncle Silas, said the doctor.  Oh, Huck, Uncle Silas. This is a good play, said Tom. I feel youre making to  start and see, Uncle Silas.  So I stuck at the trouble in a single-line voice.  No, sir, said Huck. So I went down. I came on, and they were getting  mixed up. Then I said:  Oh, sorry! Huck',\n \"Well, sir, you could do it, Tom. I should not be afraid.  Ah, mama, will you stay where we'll go?  Id have such a thing! Take the horse and use it, Tom.  Very merry, youll!  Oh, dont touch the good things for a horse, thats aint  nothing at all! Take them off!  Well, Id have, Tom, but thats an idea, Im only fine; youll _need_ more; _oh_\",\n 'Well, sir, you couldnt go anywhere but one is  that aint anything; but besides you dont even know anything about the  place. Ill know that theyre to be from another. You dont know _that_ theyre.  They are all lying to themselves. Who is any of them to, and he said, to me, it is _they_ dont know. Theyre so good about  you, and they know you all, and youre going to do,  for I was a littl']"},"metadata":{}}]},{"cell_type":"code","source":"generate_messages(\n    model,\n    tokenizer,\n    'uncle into a thousand times',\n    stop_token,\n    length,\n    num_return_sequences,\n    temperature = temperature,\n    k=k,\n    p=p,\n    repetition_penalty = repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:44:31.515995Z","iopub.execute_input":"2023-05-16T17:44:31.516734Z","iopub.status.idle":"2023-05-16T17:44:32.597016Z","shell.execute_reply.started":"2023-05-16T17:44:31.516698Z","shell.execute_reply":"2023-05-16T17:44:32.596069Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[\"uncle into a thousand times!  Who is he glad to?  Tell me about the words they gave the money; it was a thought.  Then then he said:  I think you must see you say.  Your mind is too troubled to think of this strange truth, but I doubt  there was nothing wrong about it; so I knew I believed what I had done. What else  was you referring to that?  A little ago I didn't think of that, either.  Now it was \",\n \"uncle into a thousand times.  I asked Sid to pay a visit for a letter to you from Langdon to  Elmira, but he refused to give you any more. I said:  What's the meaning of that letter to St. Petersburg.  That was a generous and happy trip from my office, but to have it  and thank you.  Elmira was ready for to propose you a month. She said she had  been with him more often than three times that year, when the talk\",\n 'uncle into a thousand times was  always there as great an object of novelty as such as a thought.  It could have been a thing like the sun, and would have been  the envy of the country; and that he lived in the solitude.  We saw he was a good man; he never remained so busy. He  never wore a rippled shirt--always in a fashion that was  better, he was above the heart of the world, and every  little change on the look of the worl',\n 'uncle into a thousand times.  Why, he was  all dead.I sat in my seat with my little and merry for a few  minutes.I whispered to my brother brother, that you ought to answer them, because you  are all dead.I mean; no matter, what the others know, your brother said,  And why are you all dead?  Well, there was no place for you?Why, not for each among us;  you are all dead.  A light about you',\n 'uncle into a thousand times from now, and he  did not wish it on, though the boy boy was quite willing  to add something to his opinion of it.  Poor little girl kept looking ahead to the beginning of a day--a time  of an old age.  Clemens would let his words go out of his words, too, but that was  a happy time. It would give a happy evening  that he would set in a fresh spirit of becoming a recluse, the result  of hi']"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# from ...tokenization_utils import PreTrainedTokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom filelock import FileLock\nfrom torch.utils.data import Dataset\n\nclass TextDataset(Dataset):\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        file_path: str,\n        block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n\n        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            cache_dir if cache_dir is not None else directory,\n            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n        )\n\n        # Make sure only the first process in distributed training processes the dataset,\n        # and the others will use the cache.\n        lock_path = cached_features_file + \".lock\"\n        with FileLock(lock_path):\n            if os.path.exists(cached_features_file) and not overwrite_cache:\n                start = time.time()\n                with open(cached_features_file, \"rb\") as handle:\n                    self.examples = pickle.load(handle)\n                logger.info(\n                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n                )\n\n            else:\n                logger.info(f\"Creating features from dataset file at {directory}\")\n\n                self.examples = []\n                with open(file_path, encoding=\"utf-8\") as f:\n                    text = f.read()\n\n                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\n                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n                    self.examples.append(\n                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n                    )\n                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n                # If your dataset is small, first you should look for a bigger one :-) and second you\n                # can change this behavior by adding (model specific) padding.\n\n                start = time.time()\n                with open(cached_features_file, \"wb\") as handle:\n                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                logger.info(\n                    f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n                )\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> torch.Tensor:\n        return torch.tensor(self.examples[i], dtype=torch.long)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LineByLineTextDataset(Dataset):\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n        # Here, we do not cache the features, operating under the assumption\n        # that we will soon use fast multithreaded tokenizers from the\n        # `tokenizers` repo everywhere \n        logger.info(f\"Creating features from dataset file at {file_path}\")\n\n        with open(file_path, encoding=\"utf-8\") as f:\n            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n\n        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n        self.examples = batch_encoding[\"input_ids\"]\n        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n        return self.examples[i]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import random\n# import warnings\n# from collections.abc import Mapping\n# from dataclasses import dataclass\n# from random import randint\n# from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n\n# import numpy as np\n\n# from ..models.bert import BertTokenizer, BertTokenizerFast\n# from ..tokenization_utils_base import PreTrainedTokenizerBase\n# from ..utils import PaddingStrategy\n\nclass DataCollatorMixin:\n    def __call__(self, features, return_tensors=None):\n        if return_tensors is None:\n            return_tensors = self.return_tensors\n        if return_tensors == \"tf\":\n            return self.tf_call(features)\n        elif return_tensors == \"pt\":\n            return self.torch_call(features)\n        elif return_tensors == \"np\":\n            return self.numpy_call(features)\n        else:\n            raise ValueError(f\"Framework '{return_tensors}' not recognized!\")\n\n\ndef default_data_collator(features: List[InputDataClass], return_tensors=\"pt\") -> Dict[str, Any]:\n    \"\"\"\n    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n    potential keys named:\n\n        - `label`: handles a single value (int or float) per object\n        - `label_ids`: handles a list of values per object\n\n    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n    to the model. See glue and ner for example of how it's useful.\n    \"\"\"\n\n    # In this function we'll make the assumption that all `features` in the batch\n    # have the same attributes.\n    # So we will look at the first element as a proxy for what attributes exist\n    # on the whole batch.\n\n    if return_tensors == \"pt\":\n        return torch_default_data_collator(features)\n    elif return_tensors == \"tf\":\n        return tf_default_data_collator(features)\n    elif return_tensors == \"np\":\n        return numpy_default_data_collator(features)\n\n    \nclass DataCollatorForLanguageModeling(DataCollatorMixin):\n    \"\"\"\n    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n    are not all of the same length.\n\n    Args:\n        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n            The tokenizer used for encoding the data.\n        mlm (`bool`, *optional*, defaults to `True`):\n            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs\n            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked\n            tokens and the value to predict for the masked token.\n        mlm_probability (`float`, *optional*, defaults to 0.15):\n            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value.\n        return_tensors (`str`):\n            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n\n    <Tip>\n\n    For best performance, this data collator should be used with a dataset having items that are dictionaries or\n    BatchEncoding, with the `\"special_tokens_mask\"` key, as returned by a [`PreTrainedTokenizer`] or a\n    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.\n\n    </Tip>\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    mlm: bool = True\n    mlm_probability: float = 0.15\n    pad_to_multiple_of: Optional[int] = None\n    tf_experimental_compile: bool = False\n    return_tensors: str = \"pt\"\n\n    def __post_init__(self):\n        if self.mlm and self.tokenizer.mask_token is None:\n            raise ValueError(\n                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n                \"You should pass `mlm=False` to train on causal language modeling instead.)\n\n    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n        # Handle dict or lists with proper padding and conversion to tensor.\n        if isinstance(examples[0], Mapping):\n            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n        else:\n            batch = {\n                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n            }\n\n        # If special token mask has been preprocessed, pop it from the dict.\n        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n        if self.mlm:\n            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n            )\n        else:\n            labels = batch[\"input_ids\"].clone()\n            if self.tokenizer.pad_token_id is not None:\n                labels[labels == self.tokenizer.pad_token_id] = -100\n            batch[\"labels\"] = labels\n        return batch\n\n    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n        \"\"\"\n        import torch\n\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = [\n                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n            ]\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataCollatorForSeq2Seq:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received, as well as the labels.\n\n    Args:\n        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n            The tokenizer used for encoding the data.\n        model ([`PreTrainedModel`]):\n            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n            prepare the *decoder_input_ids*\n\n            This is useful when using *label_smoothing* to avoid calculating loss twice.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n\n            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n              sequence is provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n        max_length (`int`, *optional*):\n            Maximum length of the returned list and optionally padding length (see above).\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value.\n\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n        label_pad_token_id (`int`, *optional*, defaults to -100):\n            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n        return_tensors (`str`):\n            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    model: Optional[Any] = None\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features, return_tensors=None):\n        if return_tensors is None:\n            return_tensors = self.return_tensors\n        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n        # same length to return tensors.\n        if labels is not None:\n            max_label_length = max(len(l) for l in labels)\n            if self.pad_to_multiple_of is not None:\n                max_label_length = (\n                    (max_label_length + self.pad_to_multiple_of - 1)\n                    // self.pad_to_multiple_of\n                    * self.pad_to_multiple_of\n                )\n\n            padding_side = self.tokenizer.padding_side\n            for feature in features:\n                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n                if isinstance(feature[\"labels\"], list):\n                    feature[\"labels\"] = (\n                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n                    )\n                elif padding_side == \"right\":\n                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n                else:\n                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n\n        features = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=return_tensors,\n        )\n\n        # prepare decoder_input_ids\n        if (\n            labels is not None\n            and self.model is not None\n            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n        ):\n            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n            features[\"decoder_input_ids\"] = decoder_input_ids\n\n        return features","metadata":{},"execution_count":null,"outputs":[]}]}
# Transformer NLP

## Demos

Here are a few info:

 In Natural Language Processing:
- [Transformer Utilization](https://github.com/Ajax0564/TransformerNLP/blob/main/utilizing-transformer-representations-efficiently.ipynb)
- [Masked language modeling with DebertaV3](https://github.com/Ajax0564/TransformerNLP/blob/main/debertav3-mlm.ipynb)
- [Training any NLP Transformer with Native Pytorch](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-pytorch-native.ipynb)
- [Roberta distilation](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-distilation-pre.ipynb)
- [Roberta Distilation classification](https://github.com/Ajax0564/TransformerNLP/blob/main/roberta-base-6L-distill-pytorch.ipynb)
- [Roberta Distilation Seq2Seq Summarization](https://github.com/Ajax0564/TransformerNLP/blob/main/robert6l-seq2seq.ipynb)
- [Knowledge distilation from large language model](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-pytorch-native-knowledge-distillation.ipynb)


## Model architectures

1. **[RoBERTa](https://arxiv.org/abs/1907.11692)** (from Facebook)

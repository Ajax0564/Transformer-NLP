# Transformer NLP

## Demos

Here are a few info:

 In Natural Language Processing:
 ## Encoder Only
- [Transformer Utilization](https://github.com/Ajax0564/TransformerNLP/blob/main/utilizing-transformer-representations-efficiently.ipynb)
- [Roberta Base Implementation](https://github.com/Ajax0564/TransformerNLP/blob/main/roberta-base-from-scratch-pytorch.ipynb)
- [Masked language modeling with DebertaV3](https://github.com/Ajax0564/TransformerNLP/blob/main/debertav3-mlm.ipynb)
- [Training any NLP Transformer with Native Pytorch](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-pytorch-native.ipynb)
- [Roberta distilation](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-distilation-pre.ipynb)
- [Roberta Distilation classification](https://github.com/Ajax0564/TransformerNLP/blob/main/roberta-base-6L-distill-pytorch.ipynb)
- [Knowledge distilation from large language model](https://github.com/Ajax0564/TransformerNLP/blob/main/transformer-pytorch-native-knowledge-distillation.ipynb)

## Decoder Only
-[GPT-2 Finetunning for text genration](https://github.com/Ajax0564/TransformerNLP/blob/main/gpt2-text-generation.ipynb)
-[GPT-2 implementation from scratch for text genration](https://github.com/Ajax0564/TransformerNLP/blob/main/gpt2-scratch-implementation.ipynb)

## Encoder Decoder
- [Roberta Distilation Seq2Seq Summarization](https://github.com/Ajax0564/TransformerNLP/blob/main/robert6l-seq2seq.ipynb)
- 
## Model architectures

1. **[RoBERTa](https://arxiv.org/abs/1907.11692)** (from Facebook)

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport torch\nfrom torch.utils.data import Dataset\nimport pickle\nimport os\nimport time \nfrom time import sleep\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom logging import Logger as logger\nimport warnings\nimport gc\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.simplefilter('ignore')\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead\nfrom tqdm.notebook import tqdm\nfrom transformers import TextDataset,DataCollatorForLanguageModeling,AutoConfig","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:02.082720Z","iopub.execute_input":"2023-05-28T10:56:02.083070Z","iopub.status.idle":"2023-05-28T10:56:12.761163Z","shell.execute_reply.started":"2023-05-28T10:56:02.083040Z","shell.execute_reply":"2023-05-28T10:56:12.760173Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"m = AutoModelWithLMHead.from_pretrained(\"../input/transformer-distilation-gpt-2/gpt2_6L\")\nconfig = AutoConfig.from_pretrained('../input/transformer-distilation-gpt-2/gpt2_6L')","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:12.763212Z","iopub.execute_input":"2023-05-28T10:56:12.763936Z","iopub.status.idle":"2023-05-28T10:56:17.581260Z","shell.execute_reply.started":"2023-05-28T10:56:12.763898Z","shell.execute_reply":"2023-05-28T10:56:17.580306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"state_dict = m.state_dict()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:17.582509Z","iopub.execute_input":"2023-05-28T10:56:17.582849Z","iopub.status.idle":"2023-05-28T10:56:17.590067Z","shell.execute_reply.started":"2023-05-28T10:56:17.582816Z","shell.execute_reply":"2023-05-28T10:56:17.589156Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"string = open('../input/mark-twain-books/MarkTwain_9_clean.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('�', '', string)\nopen('Test.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:17.592846Z","iopub.execute_input":"2023-05-28T10:56:17.593664Z","iopub.status.idle":"2023-05-28T10:56:17.613249Z","shell.execute_reply.started":"2023-05-28T10:56:17.593629Z","shell.execute_reply":"2023-05-28T10:56:17.612435Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"136454"},"metadata":{}}]},{"cell_type":"code","source":"string = open('/kaggle/input/mark-twain-books/Combine.txt',encoding='utf8',errors='ignore').read()\nnew_str = re.sub('�', '', string)\nopen('Train.txt', 'w').write(new_str)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:17.614782Z","iopub.execute_input":"2023-05-28T10:56:17.615650Z","iopub.status.idle":"2023-05-28T10:56:17.680035Z","shell.execute_reply.started":"2023-05-28T10:56:17.615617Z","shell.execute_reply":"2023-05-28T10:56:17.679129Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6588596"},"metadata":{}}]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,config,device):\n        super(MultiHeadAttention,self).__init__()\n        self.n_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.hidden_size//self.n_heads\n        self.q = nn.Linear(self.hidden_size,self.hidden_size)\n        self.k = nn.Linear(self.hidden_size,self.hidden_size)\n        self.v = nn.Linear(self.hidden_size,self.hidden_size)\n        self.device = device\n\n        self.fc = nn.Linear(self.hidden_size,self.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(self.device)\n\n    def forward(self, query, key, value, mask = None):\n        batch_size = query.shape[0]\n        Q = self.q(query)\n        K = self.k(key)\n        V = self.v(value)\n        # [batch size, query len, hid dim]\n        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        # [batch size, n heads, query len, head dim]\n        score = torch.matmul(Q, K.permute(0, 1, 3, 2)) /self.scale\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -1e10)\n        attention = torch.softmax(score, dim = -1)\n        x = torch.matmul(self.dropout(attention), V)\n        x = x.permute(0, 2, 1, 3).contiguous()\n        \n        #x = [batch size, query len, n heads, head dim]\n        \n        x = x.view(batch_size, -1, self.hidden_size)\n        \n        #x = [batch size, query len, hid dim]\n        \n        x = self.fc(x)\n        return x, attention\n    \nclass PositionwiseFeedforwardLayer(nn.Module):\n    def __init__(self,config):\n        super(PositionwiseFeedforwardLayer,self).__init__()\n        self.pf_dim = 3072\n        self.hid_dim = config.hidden_size\n        self.fc_1 = nn.Linear(self.hid_dim, self.pf_dim)\n        self.fc_2 = nn.Linear(self.pf_dim, self.hid_dim)\n        self.dropout = nn.Dropout(0.1)\n        self.activation = nn.GELU()\n        \n    def forward(self, x):\n        \n        #x = [batch size, seq len, hid dim]\n        \n        x = self.dropout(self.activation(self.fc_1(x)))\n        \n        #x = [batch size, seq len, pf dim]\n        \n        x = self.fc_2(x)\n        \n        #x = [batch size, seq len, hid dim]\n        \n        return x\n    \nclass DecoderLayer(nn.Module):\n    def __init__(self,config,device):\n        super(DecoderLayer,self).__init__()\n        self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.enc_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.ff_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.self_attention = MultiHeadAttention(config, device)\n        self.encoder_attention = MultiHeadAttention(config, device)\n        self.positionwise_feedforward = PositionwiseFeedforwardLayer(config)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, trg,trg_mask):\n        \n        #trg = [batch size, trg len, hid dim]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n        \n        #self attention\n        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n        \n        #dropout, residual connection and layer norm\n        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n            \n        #trg = [batch size, trg len, hid dim]\n            \n        #encoder attention cross attention\n    \n        \n        #dropout, residual connection and layer norm\n#         trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n                    \n        #trg = [batch size, trg len, hid dim]\n        \n        #positionwise feedforward\n        _trg = self.positionwise_feedforward(trg)\n        \n        #dropout, residual and layer norm\n        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        return trg\n    \nclass Decoder(nn.Module):\n    def __init__(self,config,device):\n        super(Decoder,self).__init__()\n        self.device = device\n        self.vocab = config.vocab_size\n        self.hid_dim = config.hidden_size\n        self.max_length = config.max_position_embeddings\n        self.tok_embedding =nn.Embedding.from_pretrained(state_dict['transformer.wte.weight'], freeze=False)                    #nn.Embedding(self.vocab, self.hid_dim)\n        self.pos_embedding = nn.Embedding.from_pretrained(state_dict['transformer.wpe.weight'], freeze=False)                     #nn.Embedding(self.max_length, self.hid_dim)\n        self.n_layers = 6 #config.num_hidden_layers\n        self.layers = nn.ModuleList([DecoderLayer(config,device) \n                                     for _ in range(self.n_layers)])\n        self.dropout = nn.Dropout(0.1)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([config.hidden_size])).to(device)\n#         self.dropout = nn.Dropout(0.1)\n        self.fc_out = nn.Linear(self.hid_dim, self.vocab)\n        \n    def make_trg_mask(self,trg):\n        \n        \n        #trg_pad_mask = [batch size, 1, 1, trg len]\n\n#         trg_pad_mask = (trg).unsqueeze(1).unsqueeze(2)  #No padding#!= tokenizer.pad_token_id\n        \n        trg_len = trg.shape[1]\n        \n        trg_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n#         print(trg_pad_mask.size(),trg_sub_mask.size())\n        #trg_sub_mask = [trg len, trg len]\n            \n#         trg_mask = trg_pad_mask & trg_sub_mask\n        \n        #trg_mask = [batch size, 1, trg len, trg len]\n        \n        return trg_mask\n    \n    def forward(self, trg):\n        \n        #trg = [batch size, trg len]\n        #enc_src = [batch size, src len, hid dim]\n        #trg_mask = [batch size, 1, trg len, trg len]\n        #src_mask = [batch size, 1, 1, src len]\n                \n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        \n        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n                            \n        #pos = [batch size, trg len]\n            \n        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n        trg_mask = self.make_trg_mask(trg)      \n        #trg = [batch size, trg len, hid dim]\n        \n        for layer in self.layers:\n            trg = layer(trg,trg_mask)\n        \n        #trg = [batch size, trg len, hid dim]\n        #attention = [batch size, n heads, trg len, src len]\n        \n        output = self.fc_out(trg)\n        \n        #output = [batch size, trg len, output dim]\n            \n        return output\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:17.681430Z","iopub.execute_input":"2023-05-28T10:56:17.681954Z","iopub.status.idle":"2023-05-28T10:56:17.732880Z","shell.execute_reply.started":"2023-05-28T10:56:17.681917Z","shell.execute_reply":"2023-05-28T10:56:17.731841Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model  = Decoder(config,device)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:17.734356Z","iopub.execute_input":"2023-05-28T10:56:17.735382Z","iopub.status.idle":"2023-05-28T10:56:23.307193Z","shell.execute_reply.started":"2023-05-28T10:56:17.735249Z","shell.execute_reply":"2023-05-28T10:56:23.306169Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#  = [3,324]\n# model(x).size\n# torch.Size([3, 324, 50257])","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:23.308804Z","iopub.execute_input":"2023-05-28T10:56:23.309195Z","iopub.status.idle":"2023-05-28T10:56:23.314023Z","shell.execute_reply.started":"2023-05-28T10:56:23.309155Z","shell.execute_reply":"2023-05-28T10:56:23.313133Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntrain_path = 'Train.txt'\ntest_path = 'Test.txt'","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:23.315638Z","iopub.execute_input":"2023-05-28T10:56:23.316385Z","iopub.status.idle":"2023-05-28T10:56:31.876778Z","shell.execute_reply.started":"2023-05-28T10:56:23.316341Z","shell.execute_reply":"2023-05-28T10:56:31.875846Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"168cc7a3ddd34d60917b8a9ce7f85282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117cd5a0e9204d73943583cb08be314e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15dfa6e7a3554a3f8a796f4d507833ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd1d49bec2f541e3bab8749822768b45"}},"metadata":{}}]},{"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(\n        self,\n        tokenizer,\n        file_path: str,\n        block_size: int):\n        if os.path.isfile(file_path) is False:\n            raise ValueError(f\"Input file path {file_path} not found\")\n\n        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n        saved = False\n        cache_dir = None\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            cache_dir if cache_dir is not None else directory,\n            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n        )\n\n     \n        if os.path.exists(cached_features_file) and saved :\n                start = time.time()\n                with open(cached_features_file, \"rb\") as handle:\n                    self.examples = pickle.load(handle)\n#                 logger.info(\n#                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n#                 )\n\n        else:\n#                 logger.info(f\"Creating features from dataset file at {directory}\")\n\n                self.examples = []\n                with open(file_path, encoding=\"utf-8\") as f:\n                    text = f.read()\n\n                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n\n                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n                    self.examples.append(\n                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n                    )\n                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n                # If your dataset is small, first you should look for a bigger one :-) and second you\n                # can change this behavior by adding (model specific) padding.\n\n                start = time.time()\n                with open(cached_features_file, \"wb\") as handle:\n                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                    saved = True\n#                 logger.info(\n#                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n#                 )\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> torch.Tensor:\n        return {\"input_ids\":torch.tensor(self.examples[i], dtype=torch.long)}","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:31.880840Z","iopub.execute_input":"2023-05-28T10:56:31.881206Z","iopub.status.idle":"2023-05-28T10:56:31.892839Z","shell.execute_reply.started":"2023-05-28T10:56:31.881170Z","shell.execute_reply":"2023-05-28T10:56:31.891752Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(TextDataset(tokenizer,train_path,128),batch_size=24, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(TextDataset(tokenizer,test_path,128),batch_size=24, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:31.894450Z","iopub.execute_input":"2023-05-28T10:56:31.895011Z","iopub.status.idle":"2023-05-28T10:56:40.106638Z","shell.execute_reply.started":"2023-05-28T10:56:31.894976Z","shell.execute_reply":"2023-05-28T10:56:40.105721Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1580900 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"no_decay = ['bias', 'LayerNorm.weight','LayerNorm.bias']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:40.159104Z","iopub.execute_input":"2023-05-28T10:56:40.159717Z","iopub.status.idle":"2023-05-28T10:56:40.172241Z","shell.execute_reply.started":"2023-05-28T10:56:40.159682Z","shell.execute_reply":"2023-05-28T10:56:40.171472Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 3\naccumulation_steps = 1\nnum_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n                                    num_training_steps=num_train_optimization_steps)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:40.173613Z","iopub.execute_input":"2023-05-28T10:56:40.174196Z","iopub.status.idle":"2023-05-28T10:56:40.183550Z","shell.execute_reply.started":"2023-05-28T10:56:40.174162Z","shell.execute_reply":"2023-05-28T10:56:40.182674Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def loss_fn(labels,prediction_scores):\n    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n    labels = labels[:, 1:].contiguous()\n    loss_fct = torch.nn.CrossEntropyLoss()\n    lm_loss = loss_fct(shifted_prediction_scores.view(-1, config.vocab_size), labels.view(-1))\n    return lm_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:40.184762Z","iopub.execute_input":"2023-05-28T10:56:40.185337Z","iopub.status.idle":"2023-05-28T10:56:40.193710Z","shell.execute_reply.started":"2023-05-28T10:56:40.185304Z","shell.execute_reply":"2023-05-28T10:56:40.192963Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def valid_func(model,valid_loader):\n    model.eval()\n    bar = tqdm(valid_loader,file=sys.stdout)\n#     loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    losses = []\n    with torch.no_grad():\n        for batch_idx, (data) in enumerate(bar):\n            data =  collate(data)\n            x = data[\"input_ids\"].to(device)\n            y = data['labels'].to(device)\n            pred = model(x)  \n            \n            loss = loss_fn(y,pred)\n            losses.append(loss.item())\n           \n            bar.set_description(f'loss: {loss.item():.5f}')\n   \n    loss_valid = np.mean(losses)\n    return loss_valid","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:40.194902Z","iopub.execute_input":"2023-05-28T10:56:40.195556Z","iopub.status.idle":"2023-05-28T10:56:40.203840Z","shell.execute_reply.started":"2023-05-28T10:56:40.195523Z","shell.execute_reply":"2023-05-28T10:56:40.202927Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import sys\nbest_epoch_loss = np.inf\nmodel.to(device)\nfor epoch in range(5):\n    start_time = time.time()\n    avg_loss = 0.0\n    model.train()\n    tbar = tqdm(train_loader, file=sys.stdout)\n    loss_list = []\n    val_loss_list = []\n    tbar.set_description(f\"Epoch {epoch+1}\")\n    for step, data in enumerate(tbar):\n        data =  collate(data)\n        x = data[\"input_ids\"].to(device)\n        y = data['labels'].to(device)\n        optimizer.zero_grad()\n        pred = model(x)\n        loss = loss_fn(y,pred)\n#         loss = loss_fn(pred, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        tbar.set_postfix(loss=loss.item())\n        sleep(0.1)\n    loss_list.append(loss.detach().cpu().item())\n    avg_loss = np.round(np.mean(loss_list), 4)\n    tbar.set_description(f\"Epoch {epoch + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n    vloss = valid_func(model,val_loader)\n#     log_df.loc[len(log_df.index)] = [epoch+1,avg_loss,vloss]\n    print(f'Epoch--{epoch+1} ### Train loss---{avg_loss} ### Valid_Loss---{vloss}')\n#     if (step%200)==0:\n#         print(f'Train_loss={avg_loss}')\n    if vloss<best_epoch_loss:\n        best_epoch_loss = vloss\n        PATH = f\"gpt2_epoch__{epoch}.pth\"\n#         model.save_pretrained(PATH)\n        torch.save(model.state_dict(), PATH)\n#         print(f'Model Saved--epoch--{epoch+1}')\n        \n    \ndel train_loader\ndel model\ndel val_loader\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T10:56:40.205315Z","iopub.execute_input":"2023-05-28T10:56:40.205913Z","iopub.status.idle":"2023-05-28T11:13:25.761865Z","shell.execute_reply.started":"2023-05-28T10:56:40.205880Z","shell.execute_reply":"2023-05-28T11:13:25.760658Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dba1bf685a146cbac41b1847f866652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb62ab2be6924cb3a93f3da45ede4f29"}},"metadata":{}},{"name":"stdout","text":"Epoch--1 ### Train loss---6.8496 ### Valid_Loss---7.626103242238362\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b4109fac4e43b39dbae68297fe378e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d25ed2f05384652b7025da95a572bab"}},"metadata":{}},{"name":"stdout","text":"Epoch--2 ### Train loss---6.4768 ### Valid_Loss---7.299758831659953\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2dec667b31d4f30b3e34a955a4a7d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"081fc447e40e4074b53f3647c07a744f"}},"metadata":{}},{"name":"stdout","text":"Epoch--3 ### Train loss---6.5631 ### Valid_Loss---7.244344870249431\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76d0c5b06f74a99853d48133ce32cbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56a7edd0e284edabb444cdaf10819fa"}},"metadata":{}},{"name":"stdout","text":"Epoch--4 ### Train loss---6.6371 ### Valid_Loss---7.244344870249431\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8083e7bfec4e4c2c9c1dbb139f0d7167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222c50547b264d078343eaeaba67d7ac"}},"metadata":{}},{"name":"stdout","text":"Epoch--5 ### Train loss---6.3216 ### Valid_Loss---7.244344870249431\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"321"},"metadata":{}}]},{"cell_type":"code","source":"# !ls ../working/gpt2_epoch__2.pth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Decoder(config,device)\nmodel.load_state_dict(torch.load('../working/gpt2_epoch__2.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:34:06.243027Z","iopub.execute_input":"2023-05-28T11:34:06.243405Z","iopub.status.idle":"2023-05-28T11:34:07.780222Z","shell.execute_reply.started":"2023-05-28T11:34:06.243372Z","shell.execute_reply":"2023-05-28T11:34:07.779120Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:34:07.782213Z","iopub.execute_input":"2023-05-28T11:34:07.782602Z","iopub.status.idle":"2023-05-28T11:34:07.928933Z","shell.execute_reply.started":"2023-05-28T11:34:07.782563Z","shell.execute_reply":"2023-05-28T11:34:07.928019Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Decoder(\n  (tok_embedding): Embedding(50257, 768)\n  (pos_embedding): Embedding(1024, 768)\n  (layers): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (enc_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (ff_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (self_attention): MultiHeadAttention(\n        (q): Linear(in_features=768, out_features=768, bias=True)\n        (k): Linear(in_features=768, out_features=768, bias=True)\n        (v): Linear(in_features=768, out_features=768, bias=True)\n        (fc): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder_attention): MultiHeadAttention(\n        (q): Linear(in_features=768, out_features=768, bias=True)\n        (k): Linear(in_features=768, out_features=768, bias=True)\n        (v): Linear(in_features=768, out_features=768, bias=True)\n        (fc): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n        (fc_1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc_2): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc_out): Linear(in_features=768, out_features=50257, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def generate(text, max_new_tokens=512, temperature=1.0, do_sample=True, top_k=5):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        idx = tokenizer.encode(text,add_special_tokens=False, return_tensors=\"pt\")\n        idx = idx.to(device)\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= 128 else idx[:, -128:]\n            # forward the model to get the logits for the index in the sequence\n            logits = model(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, top_k)\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # either sample from the distribution or take the most likely element\n            if do_sample:\n                idx_next = torch.multinomial(probs, num_samples=1)\n            else:\n                _, idx_next = torch.topk(probs, k=1, dim=-1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:35:49.071694Z","iopub.execute_input":"2023-05-28T11:35:49.072058Z","iopub.status.idle":"2023-05-28T11:35:49.083443Z","shell.execute_reply.started":"2023-05-28T11:35:49.072020Z","shell.execute_reply":"2023-05-28T11:35:49.081892Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"out = generate('this of the test')","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:35:50.426585Z","iopub.execute_input":"2023-05-28T11:35:50.426945Z","iopub.status.idle":"2023-05-28T11:35:54.079026Z","shell.execute_reply.started":"2023-05-28T11:35:50.426912Z","shell.execute_reply":"2023-05-28T11:35:54.078083Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# out","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:35:58.770146Z","iopub.execute_input":"2023-05-28T11:35:58.770567Z","iopub.status.idle":"2023-05-28T11:35:58.794296Z","shell.execute_reply.started":"2023-05-28T11:35:58.770531Z","shell.execute_reply":"2023-05-28T11:35:58.793366Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"tensor([[5661,  286,  262, 1332,   11,  220,  262,  640,   11,  220,  284,  262,\n          640,   11,  290,  262,  220,  290,  340,  373,  407,  257,  220,  290,\n          339,  373,  257,  286,  340,  373,  407,   11,  290,  339,  373,  257,\n          220,  262,  220,  290,  262,  220,  262,  220,  290,  339,  373,  407,\n          284,  262,  220,  290,  257, 1310,   13,  314,  373,  257,  220,  257,\n          220,  284,  307,  257,  640,   11,  290,  339,  550,  407,  220,  262,\n          640,  286,  262,  220,  284,  262,  220,  290,  314,  550,  407,  284,\n          262, 1310,  220,  314,  550,  587,  257,  922,   11,  290,  340,   13,\n          314,  714,  307,  257,  220,  257,  220,  262,  584,  284,  307,  257,\n         1310,   11,  290,  340,  318,  262,  220,  284,  262,  220,  314,  423,\n          262,  220,  286,  465,  220,  262,  220,  314,  373,  257, 1310,  220,\n          262,  220,  262,  220,  290,  314,  373,  257, 1310,  286,  262,  584,\n           11,  290,  340,  318,   11,  290,  339,  531,  284,  257,  290,  340,\n           13,  220,  284,  262,  584,  220,  286,  257,  640,  284,  307,  287,\n          340,  318,  257,  922,  286,  465,  220,  262, 1310,  290,  339,  561,\n          307,  284,  307,  257,  220,  383,  373,  407,  220,  257,  640,   13,\n          383,  262,  640,  284,  262,  220,  314,  423,  407,  257,  922,  284,\n          262,  220,  290,  262,  220,  262,  584,   13,  383,  339,  373,  284,\n          307,  220,  286,  262,  584,   13,  220,  262,  257,  220,  284,  307,\n          257,  220,  262,  584,   13,  679,  373,  257, 1310,   13,  679,  373,\n          257,  286,  262,  220,  290,  340,  373,  407,  284,  262,  220,  286,\n          262,  220,  286,  262,  220,  284,  423,  284,  307,  220,  314,  423,\n          220,  262,  257,  640,   11,  475,  340,   13,  220,  284,  257,  220,\n          262,  584,  286,  262,  220,  314,  550,  257,  922,  286,  262,  220,\n          262,  220,  262, 1310,   11,  262,  220,  290,  339,  550,  587,  257,\n          220,  284,  262,  220,  290,  262,  220,  262,  262,  257,  286,  340,\n          373,  407,  262,  262, 1468,   13,  314,  373,  287,  257, 1310,   13,\n          220,  314,  423,  262, 1310,  220,  262,  220,  290,  339,  373,  257,\n          220,  262,   13,  220,  314,  373,  407,  257,  922,  284,  262,  220,\n          286,  262, 1310,  284,  307,  284,  465,  550,  407,  257,  290,  262,\n          257,  922,   13,  220,  286,  257,  220,  314,  373,  257,  640,   11,\n          262, 1310,   13,  220,  286,  257,  220,  314,  423,  587,  284,  307,\n          220,  290,  262,  257,  640,   11,  290,  339,  373,  257,  220,  290,\n          339,  550,  407,  284,  262, 1310,  220,  383,  262,  640,   13,  220,\n          262,  584,   13,  220,  286,  340,  373,  257, 1310,  220,  262,  220,\n          314,  373,  257,  220,  290,  257, 1310,  290,  262,  640,   11,  290,\n          314,  561,  407,  284,  262,  584,  286,  220,  262,  257,  640,   11,\n          290,  262,  584,   13,  220,  262,  220,  262,  584,   11,  290,  262,\n          220,  262, 1310,   11,  220,  314,  373,  257, 1049,  220,  262,  584,\n           13,  383,  262,  220,  262,  220,  284,  262, 1310,   11,  339,  373,\n          257,  286,  262,  584,  220,  314,  561,  407,  257,   13,  383,  220,\n          262,  640,   11,  262,  220,  286,  340,  373,  287,  262,  220,  314,\n          423,  407,  220,  314,  550,  262, 1049,   13,  632,  373,  262,  584]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(out[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-28T11:36:06.839511Z","iopub.execute_input":"2023-05-28T11:36:06.839867Z","iopub.status.idle":"2023-05-28T11:36:06.847615Z","shell.execute_reply.started":"2023-05-28T11:36:06.839836Z","shell.execute_reply":"2023-05-28T11:36:06.846571Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"'this of the test,  the time,  to the time, and the  and it was not a  and he was a of it was not, and he was a  the  and the  the  and he was not to the  and a little. I was a  a  to be a time, and he had not  the time of the  to the  and I had not to the little  I had been a good, and it. I could be a  a  the other to be a little, and it is the  to the  I have the  of his  the  I was a little  the  the  and I was a little of the other, and it is, and he said to a and it.  to the other  of a time to be in it is a good of his  the little and he would be to be a  The was not  a time. The the time to the  I have not a good to the  and the  the other. The he was to be  of the other.  the a  to be a  the other. He was a little. He was a of the  and it was not to the  of the  of the  to have to be  I have  the a time, but it.  to a  the other of the  I had a good of the  the  the little, the  and he had been a  to the  and the  the the a of it was not the the old. I was in a little.  I have the little  the  and he was a  the.  I was not a good to the  of the little to be to his had not a and the a good.  of a  I was a time, the little.  of a  I have been to be  and the a time, and he was a  and he had not to the little  The the time.  the other.  of it was a little  the  I was a  and a little and the time, and I would not to the other of  the a time, and the other.  the  the other, and the  the little,  I was a great  the other. The the  the  to the little, he was a of the other  I would not a. The  the time, the  of it was in the  I have not  I had the great. It was the other'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}